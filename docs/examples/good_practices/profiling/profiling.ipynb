{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to read the following sections of the documentation before going through this example:\n",
    "\n",
    "- [Pytorch setup](../../frameworks/pytorch_setup/index.rst)\n",
    "- [Checkpointing](../checkpointing/index.rst)\n",
    "- [Multi-gpu training](../../distributed/multi_gpu/index.rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figuring out if or where your code may be performing slower than it needs to can be complicated.\n",
    "In the present minimal example, we'll go through a basic profiling procedure that'll tackle the following:\n",
    "\n",
    "- Diagnosing if training or dataloading is the bottleneck in your code\n",
    "- Using the pytorch profiler to find additional bottlenecks\n",
    "- Potential avenues for further optimization with torch.compile, additional workers, multiple GPUs and related optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnosing a bottleneck: is it dataloading or training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to tell if your bottleneck is coming from your dataloading procedure is to run the main script, ``main.py``, with and without training.  \n",
    "Rationale being, if you run an epoch without training and the observed throughput is similar to the one you'd obtain while training, your dataloading is running at least at the speed of you training, making it comparatively slow.  \n",
    "Take a minute to make sure this makes sense, then observe the two runs below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/05/24 13:25:45] INFO: Setting up ImageNet\n",
      "Train epoch 0: 100%|████████████████████| 1.00/1.00 [00:01<00:00, 1.20s/Samples]\n",
      "[08/05/24 13:25:52] INFO: epoch 0:\n",
      "samples/s: 14.8144, \n",
      "updates/s: 0.0000, \n",
      "val_loss: 50.1568, \n",
      "val_accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "!python main.py --n-samples=20 --epochs=1 --skip-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/05/24 13:25:58] INFO: Setting up ImageNet\n",
      "Train epoch 0: 100%|█| 1.00/1.00 [00:01<00:00, 1.39s/Samples, accuracy=0, loss=7\n",
      "[08/05/24 13:26:05] INFO: epoch 0:\n",
      "samples/s: 12.8945, \n",
      "updates/s: 0.7164, \n",
      "val_loss: 17.2102, \n",
      "val_accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "!python main.py --n-samples=20 --epochs=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the throughput of the former two cells, we can determine that dataloading was the bottleneck in our code. With all other parameters being equal, training seems to go at least as fast as dataloading, suggesting that our training loop could take advantage of a faster dataloading procedure.  \n",
    "\n",
    "Are there any other bottlenecks present? Can we further optimize our code?  \n",
    "Let's take a more in-depth look with the pytorch profiler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the PyTorch profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last operation was performed manually and was rather straightforward, since we already had a notion of where to look. In reality, bottlenecks might not be as easy to identify. Having a broader view of the model's operators can be very helpful in this pursuit. Luckily for us, PyTorch provides a way to do this through its [official profiler](https://pytorch.org/tutorials/beginner/profiler.html).\n",
    "\n",
    "In this section, we'll use the PyTorch profiler to identify additional potential bottlenecks in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/05/24 14:41:48] INFO: Setting up ImageNet\n",
      "Train epoch 0:   0%|                           | 0.00/1.00 [00:00<?, ?Samples/s]STAGE:2024-08-05 14:41:53 1916965:1916965 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "Train epoch 0: 100%|████████████████████| 1.00/1.00 [00:01<00:00, 1.34s/Samples]\n",
      "[08/05/24 14:41:55] INFO: epoch 0:\n",
      "samples/s: 13.3756, \n",
      "updates/s: 0.0000, \n",
      "val_loss: 32.6367, \n",
      "val_accuracy: 0.00%\n",
      "STAGE:2024-08-05 14:41:55 1916965:1916965 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-08-05 14:41:55 1916965:1916965 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...        85.60%        1.860s        85.60%        1.860s     465.000ms       0.000us         0.00%       0.000us       0.000us      11.48 Mb      11.48 Mb           0 b           0 b             4  \n",
      "                                           aten::conv2d         0.03%     621.000us         8.18%     177.781ms       3.354ms       0.000us         0.00%       3.523ms      66.472us           0 b           0 b      92.54 Mb     196.00 Kb            53  \n",
      "                                      aten::convolution         0.04%     924.000us         8.16%     177.210ms       3.344ms       0.000us         0.00%       3.711ms      70.019us           0 b           0 b      92.54 Mb           0 b            53  \n",
      "                                     aten::_convolution         0.03%     724.000us         8.11%     176.286ms       3.326ms       0.000us         0.00%       3.711ms      70.019us           0 b           0 b      92.54 Mb           0 b            53  \n",
      "                                aten::cudnn_convolution         4.59%      99.822ms         8.08%     175.562ms       3.312ms       3.505ms        61.21%       3.711ms      70.019us           0 b           0 b      92.54 Mb      92.54 Mb            53  \n",
      "                                       cudaLaunchKernel         6.55%     142.323ms         6.55%     142.323ms     541.152us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           263  \n",
      "                                               aten::to         0.00%      26.000us         1.58%      34.290ms       5.715ms       0.000us         0.00%       1.352ms     225.333us           0 b           0 b      12.00 Mb           0 b             6  \n",
      "                                         aten::_to_copy         0.00%      71.000us         1.58%      34.264ms       6.853ms       0.000us         0.00%       1.352ms     270.400us           0 b           0 b      12.00 Mb           0 b             5  \n",
      "                                             cudaMalloc         1.19%      25.771ms         1.19%      25.771ms       2.148ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            12  \n",
      "                                    aten::empty_strided         0.13%       2.758ms         1.10%      23.876ms       4.775ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b      12.00 Mb      12.00 Mb             5  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.173s\n",
      "Self CUDA time total: 5.726ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Basic profiler setup\n",
    "!python main.py --n-samples=20 --epochs=1 --skip-training --pytorch-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Profiler run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A-ha! [Component]'s utilization seems off. Let's introduce a quick fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: main.py [-h] [--skip-training] [--num-workers NUM_WORKERS]\n",
      "               [--n-samples N_SAMPLES] [--batch-size BATCH_SIZE]\n",
      "               [--epochs EPOCHS] [--learning-rate LEARNING_RATE]\n",
      "               [--weight-decay WEIGHT_DECAY] [--use-wandb]\n",
      "               [--wandb-user WANDB_USER] [--wandb-project WANDB_PROJECT]\n",
      "               [--wandb-api-key WANDB_API_KEY] [--pytorch-profiling]\n",
      "main.py: error: unrecognized arguments: --num-batches=20\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboard'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Fix to last bottleneck\u001b[39;00m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython main.py --num-batches=20 --epochs=1 --skip-training  --num-workers=8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mload_ext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtensorboard\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--logdir ./logdir\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/venvs/docs/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2480\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2478\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2480\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2484\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/venvs/docs/lib/python3.10/site-packages/IPython/core/magics/extension.py:33\u001b[0m, in \u001b[0;36mExtensionMagics.load_ext\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_str:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UsageError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing module name.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextension_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124malready loaded\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m extension is already loaded. To reload it, use:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m module_str)\n",
      "File \u001b[0;32m~/venvs/docs/lib/python3.10/site-packages/IPython/core/extensions.py:62\u001b[0m, in \u001b[0;36mExtensionManager.load_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load an IPython extension by its module name.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03mReturns the string \"already loaded\" if the extension is already loaded,\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m\"no load function\" if the module doesn't have a load_ipython_extension\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03mfunction, or None if it succeeded.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_str \u001b[38;5;129;01min\u001b[39;00m BUILTINS_EXTS:\n",
      "File \u001b[0;32m~/venvs/docs/lib/python3.10/site-packages/IPython/core/extensions.py:77\u001b[0m, in \u001b[0;36mExtensionManager._load_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_str \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mmodules:\n\u001b[0;32m---> 77\u001b[0m         mod \u001b[38;5;241m=\u001b[39m \u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     mod \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[module_str]\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_load_ipython_extension(mod):\n",
      "File \u001b[0;32m/cvmfs/ai.mila.quebec/apps/arch/distro/python/3.10/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1004\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorboard'"
     ]
    }
   ],
   "source": [
    "## Fix to last bottleneck\n",
    "!python main.py --num-batches=20 --epochs=1 --skip-training  --num-workers=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## New profiler run, with fixed bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? we now have a pretty telling difference in profiler outputs. Can we do any better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Show how the output of the profiler changes once this last bottleneck is fixed. Give hints as to how to keep identifying the next bottleneck, and potential avenues for further optimization (for example using something like torch.compile, or more workers, multiple GPUs, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More code changes, potential avenues for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Throughput with training\n",
    "Take a look at https://docs.mila.quebec/examples/good_practices/launch_many_jobs/index.html\n",
    "\n",
    "!srun --pty --gpus=1 --cpus-per-task=8 --mem=16G job.sh --epochs=1 --n-samples=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[PyTorch Recipes: PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)  \n",
    "[PyTorch profiler with tensorboard](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html)  \n",
    "[PyTorch End-To-End profiling](https://www.kaggle.com/code/wkaisertexas/pytorch-end-to-end-profiling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
