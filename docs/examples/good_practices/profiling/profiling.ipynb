{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to read the following sections of the documentation before going through this example:\n",
    "\n",
    "- [Pytorch setup](../../frameworks/pytorch_setup/index.rst)\n",
    "- [Checkpointing](../checkpointing/index.rst)\n",
    "- [Multi-gpu training](../../distributed/multi_gpu/index.rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figuring out if or where your code may be performing slower than it needs to can be complicated.\n",
    "In the present minimal example, we'll go through a basic profiling procedure that'll tackle the following:\n",
    "\n",
    "- Diagnosing if training or dataloading is the bottleneck in your code\n",
    "- Using the pytorch profiler to find additional bottlenecks\n",
    "- WIP Potential avenues for further optimization with torch.compile, additional workers, multiple GPUs, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to tell if your bottleneck is coming from your dataloading procedure is to run the main script, ``main.py``, with and without training.  \n",
    "Rationale being, if you run an epoch without training and the observed throughput is similar to the one you'd obtain while training, your dataloading is running at least at the speed of you training, making it comparatively slow. Take a minute to make sure this makes sense, then observe the two runs below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/05/24 13:25:45] INFO: Setting up ImageNet\n",
      "Train epoch 0: 100%|████████████████████| 1.00/1.00 [00:01<00:00, 1.20s/Samples]\n",
      "[08/05/24 13:25:52] INFO: epoch 0:\n",
      "samples/s: 14.8144, \n",
      "updates/s: 0.0000, \n",
      "val_loss: 50.1568, \n",
      "val_accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "!python main.py --n-samples=20 --epochs=1 --skip-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/05/24 13:25:58] INFO: Setting up ImageNet\n",
      "Train epoch 0: 100%|█| 1.00/1.00 [00:01<00:00, 1.39s/Samples, accuracy=0, loss=7\n",
      "[08/05/24 13:26:05] INFO: epoch 0:\n",
      "samples/s: 12.8945, \n",
      "updates/s: 0.7164, \n",
      "val_loss: 17.2102, \n",
      "val_accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "!python main.py --n-samples=20 --epochs=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3010376166.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    Take a look at https://docs.mila.quebec/examples/good_practices/launch_many_jobs/index.html\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Throughput with training\n",
    "Take a look at https://docs.mila.quebec/examples/good_practices/launch_many_jobs/index.html\n",
    "\n",
    "!srun --pty --gpus=1 --cpus-per-task=8 --mem=16G job.sh --epochs=1 --n-samples=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the throughput of the former two cells, we can determine that dataloading was/wasn't the bottleneck.  \n",
    "Did we leave any money on the table? Let's take a more in-depth look with the pytorch profiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic profiler setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Profiler run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A-ha! [Component]'s utilization seems off. Let's introduce a quick fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix to last bottleneck\n",
    "\n",
    "#!python main.py --num-batches=20 --epochs=1 --skip-training  --num-workers=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## New profiler run, with fixed bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? we now have a pretty telling difference in profiler outputs. Can we do any better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Show how the output of the profiler changes once this last bottleneck is fixed. Give hints as to how to keep identifying the next bottleneck, and potential avenues for further optimization (for example using something like torch.compile, or more workers, multiple GPUs, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More code changes, potential avenues for improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
