{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Figure out how to add links to other parts of the Mila documentation from within the notebook, to include past headers such as:  \n",
    "Prerequisites Make sure to read the following sections of the documentation before using this example:\n",
    "\n",
    "[THIS EXAMPLE](/examples/frameworks/pytorch_setup/index)\n",
    "\n",
    "* :doc:`/examples/frameworks/pytorch_setup/index`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figuring out where your code may be performing slower than it needs to can be a contrived process. Fear not! There's ways to go about this.  \n",
    "In the present minimal example, we'll go through a basic profiling example that'll tackle the following:\n",
    "- Diagnosing if training or dataloading is the bottleneck in your code\n",
    "- Using the pytorch profiler to find additional bottlenecks\n",
    "- WIP Potential avenues for further optimization with torch.compile, additional workers, multiple GPUs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[07/18/24 16:38:29]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m INFO:__main__:Setting up ImageNet        \u001b]8;id=978877;file:///home/mila/c/cesar.valdez/idt/mila-docs/docs/examples/good_practices/profiling/main.py\u001b\\\u001b[2mmain.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=611693;file:///home/mila/c/cesar.valdez/idt/mila-docs/docs/examples/good_practices/profiling/main.py#60\u001b\\\u001b[2m60\u001b[0m\u001b]8;;\u001b\\\n",
      "Train epoch 0: 100%|████████████████████| 1.00/1.00 [00:00<00:00, 1.36Samples/s]\n",
      "\u001b[2;36m[07/18/24 16:38:35]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m INFO:__main__:epoch \u001b[1;36m0\u001b[0m: samples/s:       \u001b]8;id=911051;file:///home/mila/c/cesar.valdez/idt/mila-docs/docs/examples/good_practices/profiling/main.py\u001b\\\u001b[2mmain.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=227099;file:///home/mila/c/cesar.valdez/idt/mila-docs/docs/examples/good_practices/profiling/main.py#137\u001b\\\u001b[2m137\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m24.31625459165844\u001b[0m,updates/s: \u001b[1;36m0.0\u001b[0m,       \u001b[2m           \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         val_loss: \u001b[1;36m53.850\u001b[0m, val_accuracy: \u001b[1;36m0.00\u001b[0m%   \u001b[2m           \u001b[0m\n",
      "{\"samples/s\": 24.31625459165844, \"updates/s\": 0.0, \"val_loss\": 53.849586486816406, \"val_accuracy\": 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Show what we changed about main.py? (the important bits, the added metrics for example.)\n",
    "!python main.py --n-samples=20 --epochs=1 --skip-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[07/18/24 16:41:29]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m INFO:__main__:Setting up ImageNet        \u001b]8;id=74399;file:///home/mila/c/cesar.valdez/idt/mila-docs/docs/examples/good_practices/profiling/main.py\u001b\\\u001b[2mmain.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=515519;file:///home/mila/c/cesar.valdez/idt/mila-docs/docs/examples/good_practices/profiling/main.py#60\u001b\\\u001b[2m60\u001b[0m\u001b]8;;\u001b\\\n",
      "Train epoch 0: 100%|█| 1.00/1.00 [00:01<00:00, 1.84s/Samples, accuracy=0, loss=7\n",
      "\u001b[2;36m[07/18/24 16:41:35]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m INFO:__main__:epoch \u001b[1;36m0\u001b[0m: samples/s:       \u001b]8;id=450668;file:///home/mila/c/cesar.valdez/idt/mila-docs/docs/examples/good_practices/profiling/main.py\u001b\\\u001b[2mmain.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=53134;file:///home/mila/c/cesar.valdez/idt/mila-docs/docs/examples/good_practices/profiling/main.py#137\u001b\\\u001b[2m137\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m9.782449438915627\u001b[0m,updates/s:            \u001b[2m           \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1;36m0.5434694132730904\u001b[0m, val_loss: \u001b[1;36m8.047\u001b[0m,    \u001b[2m           \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         val_accuracy: \u001b[1;36m0.00\u001b[0m%                     \u001b[2m           \u001b[0m\n",
      "{\"samples/s\": 9.782449438915627, \"updates/s\": 0.5434694132730904, \"val_loss\": 8.047250747680664, \"val_accuracy\": 0.0}\n"
     ]
    }
   ],
   "source": [
    "## Imports, setup and the like\n",
    "!python main.py --n-samples=20 --epochs=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Throughput with training\n",
    "Take a look at https://docs.mila.quebec/examples/good_practices/launch_many_jobs/index.html\n",
    "\n",
    "!srun --pty --gpus=1 --cpus-per-task=8 --mem=16G job.sh --epochs=1 --n-samples=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the throughput of the former two cells, we can determine that dataloading was/wasn't the bottleneck.  \n",
    "Did we leave any money on the table? Let's take a more in-depth look with the pytorch profiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic profiler setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Profiler run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A-ha! [Component]'s utilization seems off. Let's introduce a quick fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix to last bottleneck\n",
    "\n",
    "#!python main.py --num-batches=20 --epochs=1 --skip-training  --num-workers=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## New profiler run, with fixed bottleneck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? we now have a pretty telling difference in profiler outputs. Can we do any better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Show how the output of the profiler changes once this last bottleneck is fixed. Give hints as to how to keep identifying the next bottleneck, and potential avenues for further optimization (for example using something like torch.compile, or more workers, multiple GPUs, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More code changes, potential avenues for improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
