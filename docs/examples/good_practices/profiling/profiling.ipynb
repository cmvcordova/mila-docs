{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to read the following sections of the documentation before going through this example:\n",
    "\n",
    "- [Pytorch setup](../../frameworks/pytorch_setup/index.rst)\n",
    "- [Checkpointing](../checkpointing/index.rst)\n",
    "- [Multi-gpu training](../../distributed/multi_gpu/index.rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figuring out if or where your code may be performing slower than it needs to can be complicated.\n",
    "In the present minimal example, we'll go through a basic profiling procedure that'll tackle the following:\n",
    "\n",
    "- Diagnosing if training or dataloading is the bottleneck in your code\n",
    "- Using the pytorch profiler to find additional bottlenecks\n",
    "- Potential avenues for further optimization with torch.compile, additional workers, multiple GPUs and related optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnosing a bottleneck: is it dataloading or training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to tell if your bottleneck is coming from your dataloading procedure is to run the main script, ``main.py``, with and without training.  \n",
    "Rationale being, if you run an epoch without training and the observed throughput is similar to the one you'd obtain while training, your dataloading is running at least at the speed of you training, making it comparatively slow.  \n",
    "Take a minute to make sure this makes sense, then observe the two runs below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/05/24 13:25:45] INFO: Setting up ImageNet\n",
      "Train epoch 0: 100%|████████████████████| 1.00/1.00 [00:01<00:00, 1.20s/Samples]\n",
      "[08/05/24 13:25:52] INFO: epoch 0:\n",
      "samples/s: 14.8144, \n",
      "updates/s: 0.0000, \n",
      "val_loss: 50.1568, \n",
      "val_accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "!python main.py --n-samples=20 --epochs=1 --skip-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/05/24 13:25:58] INFO: Setting up ImageNet\n",
      "Train epoch 0: 100%|█| 1.00/1.00 [00:01<00:00, 1.39s/Samples, accuracy=0, loss=7\n",
      "[08/05/24 13:26:05] INFO: epoch 0:\n",
      "samples/s: 12.8945, \n",
      "updates/s: 0.7164, \n",
      "val_loss: 17.2102, \n",
      "val_accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "!python main.py --n-samples=20 --epochs=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the throughput of the former two cells, we can determine that dataloading was the bottleneck in our code. With all other parameters being equal, training seems to go at least as fast as dataloading, suggesting that our training loop could take advantage of a faster dataloading procedure.  \n",
    "\n",
    "Are there any other bottlenecks present? Can we further optimize our code?  \n",
    "Let's take a more in-depth look with the pytorch profiler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the PyTorch profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last operation was performed manually and was rather straightforward, since we already had a notion of where to look. In reality, bottlenecks might not be as easy to identify. Having a broader view of the model's operators can be very helpful in this pursuit. Luckily for us, PyTorch provides a way to do this through its [official profiler](https://pytorch.org/tutorials/beginner/profiler.html).\n",
    "\n",
    "In this section, we'll use the PyTorch profiler to identify additional potential bottlenecks in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/05/24 14:41:48] INFO: Setting up ImageNet\n",
      "Train epoch 0:   0%|                           | 0.00/1.00 [00:00<?, ?Samples/s]STAGE:2024-08-05 14:41:53 1916965:1916965 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "Train epoch 0: 100%|████████████████████| 1.00/1.00 [00:01<00:00, 1.34s/Samples]\n",
      "[08/05/24 14:41:55] INFO: epoch 0:\n",
      "samples/s: 13.3756, \n",
      "updates/s: 0.0000, \n",
      "val_loss: 32.6367, \n",
      "val_accuracy: 0.00%\n",
      "STAGE:2024-08-05 14:41:55 1916965:1916965 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-08-05 14:41:55 1916965:1916965 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...        85.60%        1.860s        85.60%        1.860s     465.000ms       0.000us         0.00%       0.000us       0.000us      11.48 Mb      11.48 Mb           0 b           0 b             4  \n",
      "                                           aten::conv2d         0.03%     621.000us         8.18%     177.781ms       3.354ms       0.000us         0.00%       3.523ms      66.472us           0 b           0 b      92.54 Mb     196.00 Kb            53  \n",
      "                                      aten::convolution         0.04%     924.000us         8.16%     177.210ms       3.344ms       0.000us         0.00%       3.711ms      70.019us           0 b           0 b      92.54 Mb           0 b            53  \n",
      "                                     aten::_convolution         0.03%     724.000us         8.11%     176.286ms       3.326ms       0.000us         0.00%       3.711ms      70.019us           0 b           0 b      92.54 Mb           0 b            53  \n",
      "                                aten::cudnn_convolution         4.59%      99.822ms         8.08%     175.562ms       3.312ms       3.505ms        61.21%       3.711ms      70.019us           0 b           0 b      92.54 Mb      92.54 Mb            53  \n",
      "                                       cudaLaunchKernel         6.55%     142.323ms         6.55%     142.323ms     541.152us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           263  \n",
      "                                               aten::to         0.00%      26.000us         1.58%      34.290ms       5.715ms       0.000us         0.00%       1.352ms     225.333us           0 b           0 b      12.00 Mb           0 b             6  \n",
      "                                         aten::_to_copy         0.00%      71.000us         1.58%      34.264ms       6.853ms       0.000us         0.00%       1.352ms     270.400us           0 b           0 b      12.00 Mb           0 b             5  \n",
      "                                             cudaMalloc         1.19%      25.771ms         1.19%      25.771ms       2.148ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            12  \n",
      "                                    aten::empty_strided         0.13%       2.758ms         1.10%      23.876ms       4.775ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b      12.00 Mb      12.00 Mb             5  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.173s\n",
      "Self CUDA time total: 5.726ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Basic profiler setup\n",
    "!python main.py --n-samples=20 --epochs=1 --skip-training --pytorch-profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A-ha! [Component]'s utilization seems off. Let's introduce a quick fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08/05/24 14:49:03] INFO: Setting up ImageNet\n",
      "Train epoch 0:   0%|                           | 0.00/1.00 [00:00<?, ?Samples/s]STAGE:2024-08-05 14:49:08 1939506:1939506 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "Train epoch 0: 100%|████████████████████| 1.00/1.00 [00:00<00:00, 1.05Samples/s]\n",
      "[08/05/24 14:49:10] INFO: epoch 0:\n",
      "samples/s: 18.4885, \n",
      "updates/s: 0.0000, \n",
      "val_loss: 42.7367, \n",
      "val_accuracy: 0.00%\n",
      "STAGE:2024-08-05 14:49:10 1939506:1939506 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-08-05 14:49:10 1939506:1939506 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...        83.87%        1.758s        83.88%        1.758s     439.616ms       0.000us         0.00%       0.000us       0.000us      11.48 Mb      11.48 Mb           0 b           0 b             4  \n",
      "                                           aten::conv2d         0.10%       2.029ms         9.33%     195.664ms       3.692ms       0.000us         0.00%       3.486ms      65.774us           0 b           0 b      92.54 Mb     980.00 Kb            53  \n",
      "                                      aten::convolution         0.02%     509.000us         9.32%     195.466ms       3.688ms       0.000us         0.00%       3.716ms      70.113us           0 b           0 b      92.54 Mb           0 b            53  \n",
      "                                     aten::_convolution         0.02%     355.000us         9.30%     194.957ms       3.678ms       0.000us         0.00%       3.716ms      70.113us           0 b           0 b      92.54 Mb           0 b            53  \n",
      "                                aten::cudnn_convolution         5.16%     108.149ms         9.28%     194.602ms       3.672ms       3.510ms        60.45%       3.716ms      70.113us           0 b           0 b      92.54 Mb      92.54 Mb            53  \n",
      "                                       cudaLaunchKernel         8.15%     170.863ms         8.15%     170.863ms     649.669us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           263  \n",
      "                                            aten::relu_         0.01%     293.000us         2.16%      45.194ms     922.327us       0.000us         0.00%     192.000us       3.918us           0 b           0 b           0 b           0 b            49  \n",
      "                                       aten::clamp_min_         0.05%     993.000us         2.14%      44.901ms     916.347us     192.000us         3.31%     192.000us       3.918us           0 b           0 b           0 b           0 b            49  \n",
      "                               aten::cross_entropy_loss         0.00%      74.000us         1.13%      23.612ms      23.612ms       0.000us         0.00%       5.000us       5.000us           0 b           0 b         512 b      -8.00 Kb             1  \n",
      "                                      aten::log_softmax         0.00%      21.000us         0.92%      19.350ms      19.350ms       0.000us         0.00%       3.000us       3.000us           0 b           0 b       8.00 Kb           0 b             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.097s\n",
      "Self CUDA time total: 5.806ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Fix to last bottleneck, e.g. increase workers and see throughput go down\n",
    "!python main.py --n-samples=20 --epochs=1  --num-workers=8 --skip-training  --pytorch-profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See? we now have a pretty telling difference in profiler outputs. Can we do any better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Show how the output of the profiler changes once this last bottleneck is fixed. Give hints as to how to keep identifying the next bottleneck, and potential avenues for further optimization (for example using something like torch.compile, or more workers, multiple GPUs, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More code changes, potential avenues for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Throughput with training\n",
    "Take a look at https://docs.mila.quebec/examples/good_practices/launch_many_jobs/index.html\n",
    "\n",
    "!srun --pty --gpus=1 --cpus-per-task=8 --mem=16G job.sh --epochs=1 --n-samples=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[PyTorch Recipes: PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)  \n",
    "[PyTorch profiler with tensorboard](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html)  \n",
    "[PyTorch End-To-End profiling](https://www.kaggle.com/code/wkaisertexas/pytorch-end-to-end-profiling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
